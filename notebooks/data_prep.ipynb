{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIp5hzgWxZEx+9CkFwg15S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_lx2aaZ3a1zi","executionInfo":{"status":"ok","timestamp":1755703280583,"user_tz":-60,"elapsed":200023,"user":{"displayName":"Samuel Folarin","userId":"05378071033138584286"}},"outputId":"c6f8dd00-35c4-482a-c370-db02a8cf10ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Processing SBA datasets...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_10_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_11_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_12_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_1_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_2_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_3_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_4_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_5_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_6_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_7_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_8_240930.csv...\n","Loading /content/drive/MyDrive/NGO Project/Raw data/sba/public_up_to_150k_9_240930.csv...\n","âœ… Saved: /content/drive/MyDrive/NGO Project/Raw data/processed/sba_public_up_to_150k.parquet (140.34 MB)\n","âœ… Saved: /content/drive/MyDrive/NGO Project/Raw data/processed/sba_public_150k_plus.parquet (17.53 MB)\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3234115827.py:82: DtypeWarning: Columns (19,36,37,42) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df_foia_7a = pd.read_csv(foia_7a_file, encoding='latin-1')\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Saved: /content/drive/MyDrive/NGO Project/Raw data/processed/sba_foia_7a.parquet (19.83 MB)\n","âœ… SBA datasets processed.\n","Processing CBECS datasets...\n","Found: /content/drive/MyDrive/NGO Project/Raw data/cbecs2018_final_public.csv\n","âœ… Saved: /content/drive/MyDrive/NGO Project/Raw data/processed/cbecs2018_final_public.parquet (4.82 MB)\n","Processing OpenEI datasets...\n","Found: /content/drive/MyDrive/NGO Project/Raw data/usurdb.csv\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3234115827.py:37: DtypeWarning: Columns (3,10,29,363,393,398,403,408,413,418,438,443,448,453,458,473,478,483,493,498,503,508,518,523,528,533,543,548,553,558,563,568,573,578,583,588,593,598,603,608,613,618,623,628,633,638,643,648,653,658,663,668,673,678,683,688,693,698,703,708,711,724) have mixed types. Specify dtype option on import or set low_memory=False.\n","  return pd.read_csv(path, encoding='latin-1')\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Saved: /content/drive/MyDrive/NGO Project/Raw data/processed/openei_usurdb.parquet (10.67 MB)\n","\n","ðŸ“‚ Processed files ready for analysis:\n","cbecs2018_final_public.parquet - 4.82 MB\n","openei_usurdb.parquet - 10.67 MB\n","sba_foia_7a.parquet - 19.83 MB\n","sba_public_150k_plus.parquet - 17.53 MB\n","sba_public_up_to_150k.parquet - 140.34 MB\n"]}],"source":["# DATA CLEANING PIPELINE\n","from google.colab import drive\n","import os, glob\n","import pandas as pd\n","\n","# Mount Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Set paths\n","BASE_PATH = \"/content/drive/MyDrive/NGO Project/Raw data\"\n","SBA_PATH = f\"{BASE_PATH}/sba\"\n","PROCESSED_PATH = f\"{BASE_PATH}/processed\"\n","\n","# Create processed folder\n","os.makedirs(PROCESSED_PATH, exist_ok=True)\n","\n","# Helper functions\n","def clean_column_names(df):\n","    df.columns = (\n","        df.columns.str.strip()\n","                  .str.lower()\n","                  .str.replace(\" \", \"_\")\n","                  .str.replace(\"/\", \"_\")\n","    )\n","    return df\n","\n","def save_parquet(df, filename):\n","    path = f\"{PROCESSED_PATH}/{filename}.parquet\"\n","    df.to_parquet(path, index=False)\n","    print(f\"âœ… Saved: {path} ({os.path.getsize(path) / (1024 * 1024):.2f} MB)\")\n","\n","def load_any_file(path):\n","    if not os.path.exists(path):\n","        return None\n","    ext = os.path.splitext(path)[1].lower()\n","    if ext == \".csv\":\n","        return pd.read_csv(path, encoding='latin-1')\n","    elif ext in [\".xlsx\", \".xls\"]:\n","        return pd.read_excel(path)\n","    else:\n","        print(f\"âš  Unsupported format: {path}\")\n","        return None\n","\n","# SBA DATA CLEANING\n","print(\"Processing SBA datasets...\")\n","\n","# Public up to 150k\n","csv_files = sorted(glob.glob(f\"{SBA_PATH}/public_up_to_150k_*.csv\"))\n","cols_to_keep = [\n","    \"LoanNumber\", \"DateApproved\", \"BorrowerState\", \"LoanStatus\",\n","    \"InitialApprovalAmount\", \"CurrentApprovalAmount\", \"JobsReported\",\n","    \"NAICSCode\", \"BusinessType\"\n","]\n","\n","df_list = []\n","for file in csv_files:\n","    print(f\"Loading {file}...\")\n","    chunk = pd.read_csv(file, usecols=lambda c: c in cols_to_keep, encoding='latin-1')\n","    chunk = clean_column_names(chunk)\n","    df_list.append(chunk)\n","\n","if df_list:\n","    public_up_to_150k = pd.concat(df_list, ignore_index=True)\n","    save_parquet(public_up_to_150k, \"sba_public_up_to_150k\")\n","    del public_up_to_150k, df_list\n","else:\n","    print(\"No SBA public_up_to_150k files found.\")\n","\n","# Public 150k plus\n","file_150k_plus = f\"{SBA_PATH}/public_150k_plus_240930.csv\"\n","if os.path.exists(file_150k_plus):\n","    df_150k_plus = pd.read_csv(file_150k_plus, usecols=lambda c: c in cols_to_keep, encoding='latin-1')\n","    df_150k_plus = clean_column_names(df_150k_plus)\n","    save_parquet(df_150k_plus, \"sba_public_150k_plus\")\n","    del df_150k_plus\n","else:\n","    print(f\"File not found: {file_150k_plus}\")\n","\n","# FOIA 7a\n","foia_7a_file = f\"{SBA_PATH}/foia-7a-fy2020-present-asof-250331.csv\"\n","if os.path.exists(foia_7a_file):\n","    df_foia_7a = pd.read_csv(foia_7a_file, encoding='latin-1')\n","    df_foia_7a = clean_column_names(df_foia_7a)\n","    save_parquet(df_foia_7a, \"sba_foia_7a\")\n","    del df_foia_7a\n","else:\n","    print(f\"File not found: {foia_7a_file}\")\n","\n","# bkstudy\n","bkstudy_files = sorted(glob.glob(f\"{SBA_PATH}/bkstudy_*.csv\"))\n","for file in bkstudy_files:\n","    df_bk = pd.read_csv(file, encoding='latin-1')\n","    df_bk = clean_column_names(df_bk)\n","    base_name = os.path.basename(file).replace(\".csv\", \"\")\n","    save_parquet(df_bk, f\"sba_{base_name}\")\n","    del df_bk\n","\n","print(\"âœ… SBA datasets processed.\")\n","\n","# CBECS DATA CLEANING\n","print(\"Processing CBECS datasets...\")\n","\n","cbecs_candidates = [f for f in os.listdir(BASE_PATH) if 'cbecs2018_final_public' in f.lower()]\n","if cbecs_candidates:\n","    cbecs_file = os.path.join(BASE_PATH, cbecs_candidates[0])\n","    print(f\"Found: {cbecs_file}\")\n","    df_cbecs = load_any_file(cbecs_file)\n","    if df_cbecs is not None:\n","        df_cbecs = clean_column_names(df_cbecs)\n","        save_parquet(df_cbecs, \"cbecs2018_final_public\")\n","        del df_cbecs\n","else:\n","    print(\"No CBECS dataset found.\")\n","\n","# OpenEI DATA CLEANING\n","print(\"Processing OpenEI datasets...\")\n","\n","openei_candidates = [f for f in os.listdir(BASE_PATH) if 'usurdb' in f.lower()]\n","if openei_candidates:\n","    openei_file = os.path.join(BASE_PATH, openei_candidates[0])\n","    print(f\"Found: {openei_file}\")\n","    df_openei = load_any_file(openei_file)\n","    if df_openei is not None:\n","        df_openei = clean_column_names(df_openei)\n","        save_parquet(df_openei, \"openei_usurdb\")\n","        del df_openei\n","else:\n","    print(\"No OpenEI dataset found.\")\n","\n","# Final Check\n","print(\"\\nðŸ“‚ Processed files ready for analysis:\")\n","for file in sorted(glob.glob(f\"{PROCESSED_PATH}/*.parquet\")):\n","    size_mb = os.path.getsize(file) / (1024 * 1024)\n","    print(f\"{os.path.basename(file)} - {size_mb:.2f} MB\")"]},{"cell_type":"code","source":["# Tableau file creation\n","import pandas as pd\n","import pyarrow.dataset as ds # Import pyarrow dataset\n","import numpy as np # Import numpy\n","\n","processed_path = \"/content/drive/MyDrive/NGO Project/Raw data/processed\"\n","\n","# Load SBA data using pyarrow dataset for chunking\n","sba_file = f\"{processed_path}/sba_public_up_to_150k.parquet\"\n","use_cols = [\"borrowerstate\", \"businesstype\"]\n","\n","print(\"Aggregating SBA data in chunks using pyarrow.dataset...\")\n","\n","# Create a PyArrow dataset\n","dataset = ds.dataset(sba_file, format=\"parquet\")\n","\n","agg_region = {}\n","agg_sector = {}\n","\n","# Iterate through batches instead of chunks\n","batch_size = 100_000\n","for batch in dataset.to_batches(columns=use_cols, batch_size=batch_size):\n","    chunk = batch.to_pandas()\n","    # Adoption likelihood by state\n","    state_adoption = {\n","        \"CA\": 0.45, \"NY\": 0.40, \"MA\": 0.42,\n","        \"TX\": 0.25, \"FL\": 0.28, \"IL\": 0.30,\n","        \"WV\": 0.10, \"WY\": 0.12, \"KY\": 0.15\n","    }\n","\n","    # Adoption likelihood by sector\n","    sector_adoption = {\n","        \"Professional, Scientific, and Technical Services\": 0.35,\n","        \"Educational Services\": 0.40,\n","        \"Healthcare and Social Assistance\": 0.32,\n","        \"Manufacturing\": 0.22,\n","        \"Retail Trade\": 0.18,\n","        \"Construction\": 0.15,\n","        \"Transportation and Warehousing\": 0.12,\n","    }\n","\n","    def assign_adoption(row):\n","        state_rate = state_adoption.get(row[\"borrowerstate\"], 0.20)\n","        sector_rate = sector_adoption.get(row[\"businesstype\"], 0.20)\n","        combined_rate = (state_rate + sector_rate) / 2\n","        return np.random.rand() < combined_rate\n","\n","    chunk[\"renewable_adopted\"] = chunk.apply(assign_adoption, axis=1).astype(int)\n","\n","    # Aggregate by region/state\n","    for state, grp in chunk.groupby(\"borrowerstate\"):\n","        agg_region.setdefault(state, []).extend(grp[\"renewable_adopted\"])\n","\n","    # Aggregate by business type\n","    for sector, grp in chunk.groupby(\"businesstype\"):\n","        agg_sector.setdefault(sector, []).extend(grp[\"renewable_adopted\"])\n","\n","# Convert aggregated lists to averages\n","adoption_by_region = pd.DataFrame([\n","    {\"borrowerstate\": k, \"adoption_rate\": sum(v) / len(v)}\n","    for k, v in agg_region.items()\n","])\n","\n","adoption_by_sector = pd.DataFrame([\n","    {\"businesstype\": k, \"adoption_rate\": sum(v) / len(v)}\n","    for k, v in agg_sector.items()\n","])\n","\n","# Save for Tableau\n","adoption_by_region.to_csv(f\"{processed_path}/adoption_by_region.csv\", index=False)\n","adoption_by_sector.to_csv(f\"{processed_path}/adoption_by_sector.csv\", index=False)\n","\n","print(\"âœ… Exported:\")\n","print(\"- adoption_by_region.csv\")\n","print(\"- adoption_by_sector.csv\")\n","\n","# Save merged_for_tableau with only aggregated data\n","# Note: This merge is done on the aggregated data, not the full dataset\n","# It's a cross merge here\n","merged_for_tableau = adoption_by_region.merge(adoption_by_sector, how=\"cross\")\n","merged_for_tableau.to_csv(f\"{processed_path}/merged_for_tableau.csv\", index=False)\n","\n","print(\"âœ… Merged Tableau file saved.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FKwxncCxxTk9","executionInfo":{"status":"ok","timestamp":1755703493528,"user_tz":-60,"elapsed":95645,"user":{"displayName":"Samuel Folarin","userId":"05378071033138584286"}},"outputId":"71ac095f-c024-4049-8f6e-7f50df2211aa"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Aggregating SBA data in chunks using pyarrow.dataset...\n","âœ… Exported:\n","- adoption_by_region.csv\n","- adoption_by_sector.csv\n","âœ… Merged Tableau file saved.\n"]}]}]}